{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect metadata and store it in a MongoDB instance \n",
    "This Notebook assumes that you have a list of DOIs that link to datasets of which you want to extract the metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MongoDB instance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Replace with your MongoDB connection string\n",
    "CONNECTION_STRING = \"mongodb://127.0.0.1:27018\"  # For local MongoDB\n",
    "\n",
    "# Connect to the database and collection\n",
    "client = MongoClient(CONNECTION_STRING)\n",
    "db = client[\"archaeology_metadata\"] # Create new database\n",
    "collection = db[\"collection\"] # Create new collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/archaeology_metadata.csv')\n",
    "dois = df.dsPersistentId.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_json(doi): \n",
    "    \"\"\"\n",
    "    Get JSON data of a dataset from the Archaeology Data Stations API.\n",
    "\n",
    "    :param doi: DOI of the dataset \n",
    "    :return: JSON data of the dataset \n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"https://archaeology.datastations.nl/api/datasets/export?exporter=OAI_ORE&persistentId={doi}\"\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the JSON data\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_parallel(dois, batch_size=50, max_workers=5):\n",
    "    \"\"\"\n",
    "    Process DOIs in parallel using ThreadPoolExecutor.\n",
    "\n",
    "    :param dois: List of DOIs to process\n",
    "    :param batch_size: Number of DOIs to process in each batch\n",
    "    :param max_workers: Number of threads to use for parallel processing\n",
    "    \"\"\"\n",
    "    for i in range(0, len(dois), batch_size):\n",
    "        batch = dois[i:i + batch_size]\n",
    "        #print(f\"Processing batch {i // batch_size + 1} (DOIs {i} to {i + len(batch) - 1})...\")\n",
    "\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit tasks for all DOIs in the current batch\n",
    "            future_to_doi = {executor.submit(get_json, doi): doi for doi in batch}\n",
    "\n",
    "            for future in as_completed(future_to_doi):\n",
    "                doi = future_to_doi[future]\n",
    "                try:\n",
    "                    data = future.result()\n",
    "                    if data is not None:  # Only add valid data\n",
    "                        results.append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while processing DOI {doi}: {e}\")\n",
    "\n",
    "        # Insert the batch into MongoDB\n",
    "        if results:\n",
    "            collection.insert_many(results)\n",
    "            #print(f\"Inserted {len(results)} documents into MongoDB.\")\n",
    "\n",
    "        # Optional: Add a delay between batches to respect API limits\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the batch processing function\n",
    "process_parallel(dois, batch_size=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
